{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 详细GPU诊断 ===\n",
      "PyTorch版本: 2.7.1+cu118\n",
      "正在检查CUDA可用性...\n",
      "CUDA可用: True\n",
      "CUDA设备数量: 1\n",
      "当前CUDA设备: 0\n",
      "GPU 0: NVIDIA GeForce RTX 2080\n",
      "  总内存: 8.0 GB\n",
      "  计算能力: 7.5\n",
      "  多处理器数量: 46\n",
      "\n",
      "CUDA_VISIBLE_DEVICES: 未设置\n",
      "\n",
      "=== 使用nvidia-smi检查系统GPU ===\n",
      "nvidia-smi 输出:\n",
      "Tue Oct  7 13:39:21 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.41       Driver Version: 471.41       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 41%   46C    P8     6W / 225W |   2553MiB /  8192MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2204    C+G   ... iCUE 4 Software\\iCUE.exe    N/A      |\n",
      "|    0   N/A  N/A      2372    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      2556    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      9112    C+G   ...6.500.638\\XnnExternal.exe    N/A      |\n",
      "|    0   N/A  N/A     11172    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13064    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     21828    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     22420    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     22724    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     24184    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     25152    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     25492    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     25904    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26016    C+G   ...\\Autodesk AdSSO\\AdSSO.exe    N/A      |\n",
      "|    0   N/A  N/A     27408    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     27568    C+G   ...cher\\AdskAccessUIHost.exe    N/A      |\n",
      "|    0   N/A  N/A     28360    C+G   ...x64\\electron-qq-login.exe    N/A      |\n",
      "|    0   N/A  N/A     28612    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A     30392    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A     30820    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     31180    C+G   ...Files\\Tencent\\QQNT\\QQ.exe    N/A      |\n",
      "|    0   N/A  N/A     32252    C+G   ...7pnf6hceqser\\snipaste.exe    N/A      |\n",
      "|    0   N/A  N/A     34556    C+G   ...ub.ThreadedWaitDialog.exe    N/A      |\n",
      "|    0   N/A  N/A     35060    C+G   ...s\\Notepad++\\notepad++.exe    N/A      |\n",
      "|    0   N/A  N/A     36160    C+G   ...ative Cloud UI Helper.exe    N/A      |\n",
      "|    0   N/A  N/A     36480    C+G   ...updated_web\\WXWorkWeb.exe    N/A      |\n",
      "|    0   N/A  N/A     36900    C+G   ...d\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A     38408    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     39040    C+G   ...开发版_王者万象棋体验.exe    N/A      |\n",
      "|    0   N/A  N/A     39052    C+G   ...lugins\\FlutterPlugins.exe    N/A      |\n",
      "|    0   N/A  N/A     40636    C+G   ...485.94\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     44176    C+G   ...ine\\BaiduNetdiskUnite.exe    N/A      |\n",
      "|    0   N/A  N/A     46300    C+G   ..._8wekyb3d8bbwe\\Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     47240    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     48580    C+G   ...s\\Unity Hub\\Unity Hub.exe    N/A      |\n",
      "|    0   N/A  N/A     51752    C+G   ...rograms\\Cursor\\Cursor.exe    N/A      |\n",
      "|    0   N/A  N/A     53316    C+G   ...x64__adky2gkssdxte\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     55092    C+G   ...t\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A     66232    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     66856    C+G   ..._8wekyb3d8bbwe\\Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     79020    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     79824    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     80936    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     82736    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     82804    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     83912    C+G   ..._8wekyb3d8bbwe\\Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     84728    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# 详细GPU诊断\n",
    "import torch\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 添加路径以导入utils模块\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "print(\"=== 详细GPU诊断 ===\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "\n",
    "# 安全地检查CUDA可用性\n",
    "print(\"正在检查CUDA可用性...\")\n",
    "try:\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA可用: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        try:\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"CUDA设备数量: {device_count}\")\n",
    "            \n",
    "            if device_count > 0:\n",
    "                print(f\"当前CUDA设备: {torch.cuda.current_device()}\")\n",
    "                for i in range(device_count):\n",
    "                    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "                    props = torch.cuda.get_device_properties(i)\n",
    "                    print(f\"  总内存: {props.total_memory / 1024**3:.1f} GB\")\n",
    "                    print(f\"  计算能力: {props.major}.{props.minor}\")\n",
    "                    print(f\"  多处理器数量: {props.multi_processor_count}\")\n",
    "            else:\n",
    "                print(\"没有检测到CUDA设备\")\n",
    "        except Exception as e:\n",
    "            print(f\"获取CUDA设备信息时出错: {e}\")\n",
    "    else:\n",
    "        print(\"CUDA不可用\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"检查CUDA时出错: {e}\")\n",
    "    print(\"可能的原因:\")\n",
    "    print(\"1. PyTorch安装有问题\")\n",
    "    print(\"2. CUDA驱动未安装\")\n",
    "    print(\"3. PyTorch版本与CUDA不兼容\")\n",
    "\n",
    "# 检查环境变量\n",
    "print(f\"\\nCUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', '未设置')}\")\n",
    "\n",
    "# 尝试使用nvidia-smi检查\n",
    "print(\"\\n=== 使用nvidia-smi检查系统GPU ===\")\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n",
    "    if result.returncode == 0:\n",
    "        print(\"nvidia-smi 输出:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"nvidia-smi 错误: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"nvidia-smi 执行失败: {e}\")\n",
    "    print(\"可能的原因:\")\n",
    "    print(\"1. nvidia-smi 未安装\")\n",
    "    print(\"2. NVIDIA驱动未安装\")\n",
    "    print(\"3. 没有NVIDIA GPU\")\n",
    "\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch安装检查 ===\n",
      "✓ PyTorch已安装，版本: 2.7.1+cu118\n",
      "✓ PyTorch支持CUDA\n",
      "✓ torch.version 模块存在\n",
      "✓ CUDA版本: 11.8\n",
      "==================\n",
      "=== 测试GPU工具函数 ===\n",
      "query_gpu() 返回结果:\n",
      "  GPU 0: 0, NVIDIA GeForce RTX 2080, 5638 MiB\n",
      "select_gpu() 返回结果: [0]\n",
      "将使用GPU: [0]\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# PyTorch安装检查\n",
    "print(\"=== PyTorch安装检查 ===\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✓ PyTorch已安装，版本: {torch.__version__}\")\n",
    "    \n",
    "    # 检查是否有CUDA支持\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"✓ PyTorch支持CUDA\")\n",
    "    else:\n",
    "        print(\"✗ PyTorch不支持CUDA或CUDA不可用\")\n",
    "        \n",
    "    # 检查torch.version是否存在（在PyTorch 2.7.1中可能不存在）\n",
    "    if hasattr(torch, 'version'):\n",
    "        print(\"✓ torch.version 模块存在\")\n",
    "        if hasattr(torch.version, 'cuda'):\n",
    "            print(f\"✓ CUDA版本: {torch.version.cuda}\")\n",
    "        else:\n",
    "            print(\"✗ torch.version.cuda 不存在\")\n",
    "    else:\n",
    "        print(\"ℹ torch.version 模块不存在（这在PyTorch 2.7.1中是正常的）\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"✗ PyTorch未安装\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ PyTorch检查失败: {e}\")\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "# 测试GPU工具函数\n",
    "print(\"=== 测试GPU工具函数 ===\")\n",
    "try:\n",
    "    # 添加路径以导入utils模块\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.insert(0, '..')\n",
    "    \n",
    "    from utils.gpu_tools import query_gpu, select_gpu\n",
    "    \n",
    "    # 查询GPU信息\n",
    "    gpu_info = query_gpu()\n",
    "    print(f\"query_gpu() 返回结果:\")\n",
    "    for i, line in enumerate(gpu_info):\n",
    "        print(f\"  GPU {i}: {line.strip()}\")\n",
    "    \n",
    "    # 选择GPU\n",
    "    selected_gpus = select_gpu(gpu_info)\n",
    "    print(f\"select_gpu() 返回结果: {selected_gpus}\")\n",
    "    \n",
    "    if selected_gpus:\n",
    "        print(f\"将使用GPU: {selected_gpus}\")\n",
    "    else:\n",
    "        print(\"没有选择到可用的GPU\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"GPU工具函数测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU状态检查 ===\n",
      "CUDA可用: True\n",
      "GPU数量: 1\n",
      "GPU 0: NVIDIA GeForce RTX 2080\n",
      "  内存: 8.0 GB\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# GPU状态检查\n",
    "print(\"=== GPU状态检查 ===\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  内存: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"没有可用的CUDA设备\")\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "if use_gpu:\n",
    "    from utils.gpu_tools import *\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "here we choose 1993-2001 data as our training(include validation) data, the remaining will be used in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(793019, 64, 60)\n",
      "(793019, 8)\n"
     ]
    }
   ],
   "source": [
    "year_list = np.arange(1993,2001,1)\n",
    "\n",
    "images = []\n",
    "label_df = []\n",
    "for year in year_list:\n",
    "    images.append(np.memmap(os.path.join(\"../monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20])))\n",
    "    label_df.append(pd.read_feather(os.path.join(\"../monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\")))\n",
    "    \n",
    "images = np.concatenate(images)\n",
    "label_df = pd.concat(label_df)\n",
    "\n",
    "print(images.shape)\n",
    "print(label_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split method (not random split is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(images.shape[0] * 0.7)\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.Ret_20d > 0).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.Ret_20d > 0).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.Ret_20d > 0).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "export_onnx = True\n",
    "net = baseline.Net().to(device)\n",
    "net.apply(init_weights)\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    x = torch.randn([1,1,64,60]).to(device)\n",
    "    torch.onnx.export(net,               # model being run\n",
    "                      x,                         # model input (or a tuple for multiple inputs)\n",
    "                      \"../cnn_baseline.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                      export_params=False,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=10,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=False,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input_images'],   # the model's input names\n",
    "                      output_names = ['output_prob'], # the model's output names\n",
    "                      dynamic_axes={'input_images' : {0 : 'batch_size'},    # variable length axes\n",
    "                                     'output_prob' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight : torch.Size([64, 1, 5, 3])\n",
      "layer1.0.bias : torch.Size([64])\n",
      "layer1.1.weight : torch.Size([64])\n",
      "layer1.1.bias : torch.Size([64])\n",
      "layer2.0.weight : torch.Size([128, 64, 5, 3])\n",
      "layer2.0.bias : torch.Size([128])\n",
      "layer2.1.weight : torch.Size([128])\n",
      "layer2.1.bias : torch.Size([128])\n",
      "layer3.0.weight : torch.Size([256, 128, 5, 3])\n",
      "layer3.0.bias : torch.Size([256])\n",
      "layer3.1.weight : torch.Size([256])\n",
      "layer3.1.bias : torch.Size([256])\n",
      "fc1.1.weight : torch.Size([2, 46080])\n",
      "fc1.1.bias : torch.Size([2])\n",
      "total_parameters : 708866\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for name, parameters in net.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "    count += parameters.numel()\n",
    "print('total_parameters : {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "FLOPs = 36.21961728G\n",
      "Params = 0.708866M\n"
     ]
    }
   ],
   "source": [
    "from thop import profile as thop_profile\n",
    "\n",
    "flops, params = thop_profile(net, inputs=(next(iter(train_dataloader))[0].to(device),))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     146.711ms       906.39%     146.711ms     146.711ms             1  \n",
      "                                        model_inference        22.20%      46.764ms        94.69%     199.437ms     199.437ms       0.000us         0.00%      16.186ms      16.186ms             1  \n",
      "                                           aten::conv2d         0.01%      20.800us        15.19%      31.996ms      10.665ms       0.000us         0.00%      12.153ms       4.051ms             3  \n",
      "                                      aten::convolution         0.03%      58.500us        15.18%      31.975ms      10.658ms       0.000us         0.00%      12.153ms       4.051ms             3  \n",
      "                                     aten::_convolution         3.15%       6.644ms        15.15%      31.916ms      10.639ms       0.000us         0.00%      12.153ms       4.051ms             3  \n",
      "                                aten::cudnn_convolution        11.64%      24.510ms        11.96%      25.182ms       8.394ms      11.349ms        70.12%      11.349ms       3.783ms             3  \n",
      "_5x_cudnn_volta_scudnn_128x64_relu_xregs_large_nn_v1...         0.00%       0.000us         0.00%       0.000us       0.000us       8.762ms        54.13%       8.762ms       8.762ms             1  \n",
      "        _5x_cudnn_volta_scudnn_128x128_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us       2.379ms        14.70%       2.379ms       2.379ms             1  \n",
      "                                       aten::batch_norm         0.07%     154.300us        13.31%      28.028ms       9.343ms       0.000us         0.00%       1.662ms     554.002us             3  \n",
      "                           aten::_batch_norm_impl_index         2.90%       6.100ms        13.23%      27.873ms       9.291ms       0.000us         0.00%       1.662ms     554.002us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 210.616ms\n",
      "Self CUDA time total: 16.186ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "inputs = next(iter(train_dataloader))[0].to(device)\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"../trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torch.load('/home/clidg/proj_2/pt/baseline_epoch_10_train_0.6865865240322523_eval_0.686580_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到 1 个GPU\n",
      "只有一个GPU可用，不使用DataParallel\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU可用性并设置DataParallel\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    # 检查CUDA是否可用\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"警告: CUDA不可用，将使用CPU\")\n",
    "        use_gpu = False\n",
    "        device = 'cpu'\n",
    "        net = net.to(device)\n",
    "    else:\n",
    "        # 检查可用GPU数量\n",
    "        available_gpus = torch.cuda.device_count()\n",
    "        print(f\"检测到 {available_gpus} 个GPU\")\n",
    "        \n",
    "        if available_gpus == 0:\n",
    "            print(\"警告: 没有可用的GPU，将使用CPU\")\n",
    "            use_gpu = False\n",
    "            device = 'cpu'\n",
    "            net = net.to(device)\n",
    "        elif available_gpus == 1:\n",
    "            print(\"只有一个GPU可用，不使用DataParallel\")\n",
    "            net = net.to(device)\n",
    "        else:\n",
    "            print(f\"使用DataParallel，GPU数量: {available_gpus}\")\n",
    "            net = net.to(device)\n",
    "            net = nn.DataParallel(net)\n",
    "elif use_gpu:\n",
    "    net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4337/4337 [8:27:41<00:00,  7.02s/it, running_loss=0.774]       \n",
      "  0%|          | 0/930 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "os.mkdir('..\\\\pt'+os.sep+start_time)\n",
    "epochs = 100\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
